{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04. Text Classification\n",
    "\n",
    "\n",
    "This lab is devoted to text classification tasks.\n",
    "- **Part 1 [4 points]**: you will be offered to separate Russian surnames from Russian words with Vowpal Wabbit.\n",
    "- **Part 2 [11 points]** is about very common NLP problem - sentiment analysis.\n",
    "- **Part 3 [5 points]** include tasks on POS tagging and WordEmbeddings.\n",
    "\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "Each task has its value, **20 points** in total. If you use some open-source code please make sure to include the url.\n",
    "\n",
    "#### How to submit\n",
    "\n",
    "- Name your file according to this convention: `lab04_GroupNo_Surname_Name.ipynb`. If you don't have group number, put `nan` instead.\n",
    "- Attach it to an **email** with **topic** `lab04_GroupNo_Surname_Name.ipynb`\n",
    "- Send it to `cosmic.research.ml@yandex.ru`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 [4+ points]. Vowpal Wabbit vs. Russian Surnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vowpal Wabbit is machine learning model that is very efficient in text processing. You may read about it here:\n",
    "\n",
    "- [Github repo](https://github.com/JohnLangford/vowpal_wabbit/wiki)\n",
    "- [Tutorial VW](https://github.com/JohnLangford/vowpal_wabbit/wiki/Tutorial)\n",
    "- [Data Format](https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format)\n",
    "- [Command Line Parameters](https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments)\n",
    "\n",
    "The task is to separate two sets: Russian surnames and common Russian words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and prepare the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_from_file(filename, prefix=\"\"):\n",
    "    res = []\n",
    "    with open(prefix + filename, \"r\") as input_file:\n",
    "        for line in input_file.readlines():\n",
    "            res.append(line.strip())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = np.array(read_list_from_file(\"russian_surnames.txt\"))\n",
    "all_words = np.array(read_list_from_file(\"russian.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(surnames.shape, np.random.choice(surnames, 10), sep='\\n')\n",
    "print(all_words.shape, np.random.choice(all_words, 10), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames_labels = np.ones_like(surnames, dtype=int)\n",
    "allwords_labels = - np.ones_like(all_words, dtype=int)\n",
    "\n",
    "X = np.concatenate([surnames, all_words])\n",
    "y = np.concatenate([surnames_labels, allwords_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, stratify=y, random_state=42)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On VW format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the following form of vw-strings:\n",
    "`label | namespace_1 | namespace_2 | ... | namespace_k`\n",
    "\n",
    "Label is either `1` or `-1` for the binary case.\n",
    "\n",
    "Each namespace is a group of features in VW format, for example: \n",
    "- `a:1 b:2 c:1`\n",
    "- `a b b c`\n",
    "- `a:1 b:1 b:1 c:1`\n",
    "\n",
    "All these records are equivalent to VW\n",
    "\n",
    "There are ways to manually add weights, tags and names of namespaces, you may read about it here:\n",
    "https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Input-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As VW uses bag of words representations, we will use bag of letter-tokens.\n",
    "\n",
    "**Task 1.1 [1 point]**\n",
    "- Implement `tokenize_word`\n",
    "- Implement `get_token_features`\n",
    "- Answer the question: why splitting the word into 3 or 4 letter tokens might be useful in this particular task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(word, token_len=3):\n",
    "    ''' Function that splits word into sequence of tokens\n",
    "    Args:\n",
    "        word (string): input word\n",
    "        token_len (int): length of each token\n",
    "    Returns:\n",
    "        list(str): list of tokens \n",
    "    '''\n",
    "    tokens_list = []\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return tokens_list\n",
    "\n",
    "assert tokenize_word(\"cybersnatch\") == ['cyb', 'ybe', 'ber', 'ers', 'rsn', 'sna', 'nat', 'atc', 'tch'], \"smth's wrong\"\n",
    "print(\"tokenize_word: seems legit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_features(word, token_sizes=[3]):\n",
    "    ''' Function that converts a word into vw-format feature-string.\n",
    "    This feature string consists of several namespaces (1 or more), \n",
    "    each namespace corresponds to one token_size.\n",
    "    Args:\n",
    "        word (string): input word\n",
    "        token_sizes (list(int)): token lengths used for feature extraction\n",
    "    Returns:\n",
    "        str: feature-string like \"namespace_1 | namespace_2 | ... | namespace_n\"\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return # YOUR CODE HERE\n",
    "\n",
    "test_output_0 = get_token_features(\"cybersnatch\", token_sizes=[3])\n",
    "test_output_1 = get_token_features(\"cybersnatch\", token_sizes=[2, 4])\n",
    "assert test_output_0 == \"cyb ybe ber ers rsn sna nat atc tch\", \"smth's wrong\"\n",
    "assert test_output_1 == \"cy yb be er rs sn na at tc ch | cybe yber bers ersn rsna snat natc atch\", \"smth's wrong\"\n",
    "print(test_output_0)\n",
    "print(test_output_1)\n",
    "print(\"get_token_features: seems legit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't skip the question part: *Answer the question: why splitting the word into 3 or 4 letter tokens might be useful in this particular task?*\n",
    "\n",
    "**Your answer here**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you have just implemented (`get_token_features`) is an example of feature extractor. \n",
    "It takes an input word and returns a string that VW can process. \n",
    "\n",
    "As you may have noticed from VW data format, if you concatenate two feature strings (with ` | ` in the middle), you will also get a feature-string. \n",
    "We will use this fact while processing the whole dataset in vw-files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2 [1 point]** Implement a function, takes a word, its label, list of feature-extractors and makes a labelled vw-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vw_line(word, label, feature_extractors):\n",
    "    ''' Function that cooks a full vw-format string: \"label | features\".\n",
    "    Features are extracted with `feature_extractors`. \n",
    "    Result of each feature_extractor is stored in separate namespace\n",
    "    Args:\n",
    "        word (string): input word\n",
    "        label (int): its label\n",
    "        feature_extractors (list(function)): list if feature_extractor functions\n",
    "    Returns:\n",
    "        str: string like \"label | namespace_1 | namespace_2 | ... | namespace_n\"\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some dummy extractors for the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_tokenizer_1(word):\n",
    "    return word[0]\n",
    "\n",
    "def dummy_tokenizer_2(word):\n",
    "    return word[::-1]\n",
    "\n",
    "def dummy_tokenizer_3(word):\n",
    "    return get_token_features(word, [2])\n",
    "\n",
    "test_output = get_vw_line(\"test\", -1, [dummy_tokenizer_1, dummy_tokenizer_2, dummy_tokenizer_3])\n",
    "assert test_output == \"-1 | t | tset | te es st\", \"smth's wrong\"\n",
    "print(test_output)\n",
    "print(\"get_vw_line: seems legit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we are ready to prepare for the training: we will use 3 and 4 letter tokens as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(word):\n",
    "    return get_token_features(word, [3, 4])\n",
    "\n",
    "get_vw_line(\"cybersnatch\", 1, [simple_tokenizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vw_file(words, labels, out_filename, feature_extractors):\n",
    "    with open(out_filename, \"w\", encoding=\"utf8\") as out_file:\n",
    "        for word, label in zip(words, labels):\n",
    "            line = get_vw_line(word, label, feature_extractors)\n",
    "            out_file.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two files: one with train set, one with test set. It may take some time. Estimated size of train file is 200-300Mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vw_file(X_train, y_train, \"vw_train\", [simple_tokenizer])\n",
    "write_vw_file(X_test, y_test, \"vw_test\", [simple_tokenizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to install the mighty Vowpal Wabbit itself.\n",
    "\n",
    "We suggest using command line version. It is installed with the following sequence of command line commands:\n",
    "```\n",
    "git clone --recursive https://github.com/VowpalWabbit/vowpal_wabbit.git \n",
    "cd vowpal_wabbit/; make \n",
    "cd vowpal_wabbit/; make install \n",
    "```\n",
    "You may run them in jupyter terminal: use `New -> Terminal` on jupyter homepage (like here http://localhost:8888/tree/JupyterNotebooks or whatever port you are using).\n",
    "\n",
    "\n",
    "Or just run the following code in a jupyter cell of this notebook:\n",
    "```\n",
    "%%capture\n",
    "!git clone --recursive https://github.com/VowpalWabbit/vowpal_wabbit.git \n",
    "!cd vowpal_wabbit/; make \n",
    "!cd vowpal_wabbit/; make install \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !git clone --recursive https://github.com/VowpalWabbit/vowpal_wabbit.git \n",
    "# !cd vowpal_wabbit/; make \n",
    "# !cd vowpal_wabbit/; make install "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "Run the following command to train a model.\n",
    "- `-d vw_train` it learns on file \"vw_train\"\n",
    "- `--loss_function=logistic` it optimizes logloss while training\n",
    "- `-f model_v0.vw` it save the model into \"model_v0.vw\" file\n",
    "- `--quiet` it does not report anything while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw -d vw_train --loss_function=logistic -f model_v0.vw --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now collect prediction:\n",
    "- `-i model_v0.vw` - this model is used for predictions\n",
    "- `-t vw_test` - these are objects to predict on\n",
    "- `-p v0_predictions` - predictions are stored here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw -i model_v0.vw -t vw_test -p v0_predictions --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_predictions(filename):\n",
    "    with open(filename, \"r\") as pred_file:\n",
    "        predictions = [float(label) for label in pred_file.readlines()]\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since vowpal wabbit predicts some scores, we will transform them into [0, 1] floats with sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = logistic.cdf(read_predictions(\"v0_predictions\"))\n",
    "preds = np.array(pred_proba > 0.5, dtype=int) * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure quality of classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC {:.5f}\".format(roc_auc_score(y_test, pred_proba)))\n",
    "print(\"Accuracy {:.5f}\".format(accuracy_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get ROC AUC around 0.99 and accuracy around 0.95.\n",
    "Not bad, right?\n",
    "\n",
    "**Task 1.3** Improve accuracy with the following steps.\n",
    "\n",
    "**Task 1.3.1 [0.5 points]**\n",
    "Use feature ngrams while training (`--ngram`). Did they improve the score? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at this example: \n",
    "- `ivanov -> iva van ano nov | ivan vano anov`\n",
    "- It ends with `nov` which is a very common Russian surname ending.\n",
    "- However, there are words like `november`, `innovation` and many others that have the same token `nov`.\n",
    "\n",
    "**Task 1.3.2 [0.5 points]** Suggest and implement a method that will allow to separate tokens that are in the end of the word and that are not.\n",
    "\n",
    "You will have to create a new `feature_extractor` that uses this method, rewrite train- and test- files and rerun training and testing parts.\n",
    "\n",
    "Do we see any improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3.3 [1 point]** Dataset contains surnames in different forms: `иванову, ивановой, ивановых` and so on.\n",
    "\n",
    "Sometimes it is useful to normalize them before tokenization. You may use some external libs to convert every word to its normal form.\n",
    "For example, you may use this one:\n",
    "```\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "```\n",
    "You will need to update your list of feature_extractors and rewrite train- and test- files again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3.4 [extra points]**\n",
    "\n",
    "This is a place for your further experiments. Maximize Accuracy of the model.\n",
    "- **[few extra points]** use new training parameters (may the `!vw --help` help you)\n",
    "- **[many extra points]** invent new features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !vw --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Bag of Words vs. Bag of Popcorn [11 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is based on [Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) competition. The goal is to label film reviews as positive or negative. \n",
    "\n",
    "Reviews may look like this:\n",
    "\n",
    "```\n",
    "I dont know why people think this is such a bad movie. Its got a pretty good plot, some good action, and the change of location for Harry does not hurt either. Sure some of its offensive and gratuitous but this is not the only movie like that. Eastwood is in good form as Dirty Harry, and I liked Pat Hingle in this movie as the small town cop. If you liked DIRTY HARRY, then you should see this one, its a lot better than THE DEAD POOL. 4/5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"reviews.tsv\", sep=\"\\t\")\n",
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews[\"review\"]\n",
    "y = reviews[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=5000, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to extract features\n",
    "\n",
    "In this part of the assignment we will apply several methods of feature extraction and comapre them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1 [0.5 point] - Simple BOW** \n",
    "\n",
    "In this task we will build a simple bow representation - without any preprocessing. \n",
    "\n",
    "For this purpose we will use [*CountVectorizer*](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) - a method that transforms text dataset into a [sparse matrix](https://docs.scipy.org/doc/scipy/reference/sparse.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try each of these approaches:\n",
    "- fit vectorizer on X_train, apply to X_train, X_test\n",
    "- fit vectorizer on X_train, apply to X_train; fit on X_test, apply to X_test\n",
    "- fit vectorizer on X, apply to X_train, X_test\n",
    "\n",
    "Report output matrix sizes in each case. \n",
    "- What is the difference? \n",
    "- Which of these approaches is the most fair and correct?\n",
    "\n",
    "Use the most fair and correct one to get `X_train_0` and `X_test_0` - they will be needed for further tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(X_train_0.shape, X_test_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(X_train_0.shape, X_test_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(X_train_0.shape, X_test_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2 [1 point] - S___se matrices**\n",
    "\n",
    "What is the data type of `X_train_0` and `X_test_0`? What are those?\n",
    "\n",
    "What differs them from usual np.arrays? Name several types how those special matrices are stored and what they are good for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3 [1.5 points] - Training**\n",
    "\n",
    "Train LogisticRegression and Random forest on this data representations.\n",
    "- Compare training time\n",
    "- Compare Accuracy, precision, recall\n",
    "- Plot ROC Curve and calculate ROC AUC (don't forget to predict_proba)\n",
    "- Plot Precision-Recall curve and calculate f1-score (for example, with `plt.subplots(nrows=1, ncols=2)`)\n",
    "- Print the trickiest missclassified objects. Why they were hard to classify?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=500)\n",
    "lr_model = LogisticRegression(max_iter=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model gives higher scores?\n",
    "\n",
    "*Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More sophisticated feature prerocessing\n",
    "\n",
    "As we have seen, simple BOW can give us some result - it's time to improve it.\n",
    "\n",
    "**Task 2.4 [1 point] - Frequencies calculation**\n",
    "\n",
    "- Calculate top-20 words in train set and test set. *Are they meaningful?*\n",
    "- Import `stopwords` and print some of them. What are those?\n",
    "- Recalculate top-20 words in each set, but exclude stop words.\n",
    "- Does now top-20 include more useful words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5 [1 point] - Word Freqs by class**\n",
    "\n",
    "How do you think, will top100 tokens for positive and negative classes be different? Use data to prove your point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.6 [2 points] - Reducing dimensionality**\n",
    "\n",
    "The goal is to reduce number of features to 15000.\n",
    "\n",
    "Implement the following methods of dimensinality reduction:\n",
    "1. Use CountVectorizer, but leave only 15k most frequent tokens\n",
    "2. Use HashingVectorizer with 15k features\n",
    "3. Use 15k most important features from perspective of previously trained RandomForest\n",
    "\n",
    "*Hints:*\n",
    "- in 1 and 2 you don't have to apply nltk.corpus.stopwords, vectorizers have `stopwords` parameter\n",
    "- in 1 look for `vocabulary` parameter\n",
    "- in 3... remember `lab02`? You may use `X_train_0` and `X_test_0` as input matrices\n",
    "\n",
    "Train LogisticRegression and RandomForest on each dataset and compare ROC AUC scores of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.7 [2 points] - Token Normalization**\n",
    "\n",
    "Choose the best working method from previous task. Try improve it by applying a token normalization technique.\n",
    "\n",
    "You may use one of normalizers imported below, but feel free to experiment.\n",
    "\n",
    "Do the following:\n",
    "- Apply normalizer to X_train, X_test\n",
    "- Build BOW with CountVectorizer + stopwords. What are the shapes of train and test matrices now?\n",
    "- Reduce dimensionality with the best method from Task 2.6. You may try all of them\n",
    "- Train LR/RF to examine whether ROC AUC or Accuracy was improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.8 [2 points] - Vowpal Wabbit**\n",
    "\n",
    "An experiment! What if we omit all this manual feature engineering part and apply good all VW.\n",
    "\n",
    "Prepare train and test files in the following format - without any preprocessing.\n",
    "```\n",
    "1 | The film was very very good, I liked ...\n",
    "-1 | The worst film ever. Oh God ...\n",
    "...\n",
    "```\n",
    "\n",
    "Train VW with not very sophisticated parameters, compare quality to LR/RF trained on datasets above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Word Embeddings [5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of pretrained word embedding models. We suggest using `glove-wiki-gigaword-100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gensim.downloader.info()['models'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = gensim.downloader.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1 [1 point] - WordEmbeddings Geometry**\n",
    "\n",
    "As you probably know, vector space of word embeddings has non-trivial geometry: some word relations (like country-capital or single-plural) cab be represented by vectors, like: **(king - man) + woman = queen**\n",
    "\n",
    "<img src=\"https://linkme.ufanet.ru/images/5687a2011b49eb2413912f1c7d0fb0bd.png\" width=600px>\n",
    "\n",
    "Check this statement on words from the above picture with `word_embeddings.most_similar` function. Pay attention to `positive` and `negative` params.\n",
    "\n",
    "Provide **several** examples, make sure to present different relations: some for nouns, some for verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.2 [1 point] - POS analysis**\n",
    "\n",
    "Use POS tagger to calculate most common POS in the dataset. \n",
    "Here you may read about nltk-taggers: [link](https://www.inf.ed.ac.uk/teaching/courses/icl/nltk/tagging.pdf)\n",
    "\n",
    "- If you were to design POS-related weights, how would you do it?\n",
    "- What POS would get the higher weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.3 [3 points] - WordEmbeddings**\n",
    "\n",
    "Use dense vector representations to construct vector-representation of each review, then train a model (LR or RF).\n",
    "\n",
    "Compare results of the new model to results of the models above.\n",
    "**Important**\n",
    "- If you just sum embeddings of each token to get an embedding of the whole review, the cost of the task is **[2 points]**\n",
    "- For **[3 points]** you have to use either TF-IDF weight or weights that you designed from POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
